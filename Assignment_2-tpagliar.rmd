---
title: "Assignment2-Pagliari"
author: "Tim Pagliari tpagliar@kent.edu"
date: "10/1/2020"
output:
  html_document:
    df_print: paged
    toc: true
  html_notebook:
    highlight: textmate
    theme: cerulean
---

```{r setup, include=FALSE}```
##Assignment 2 for Tim Pagliari
First, we'll include the libraries used in this project:
```{r}
library(caret)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(fastDummies)
```

Now I'll load the UniversalBank.csv file which contains customer information and convert the categorical data to factors.
```{r}
BankData <- read.csv("~/GIT/MIS64060-tpagliar/UniversalBank.csv")
BankData$Personal.Loan<-factor(BankData$Personal.Loan,levels=c('0','1'),labels=c('No','Yes'))
summary(BankData)
```

##Data Selection
Now we should select our relevant data, (in this case, all but ID and Zip - we'll also convert Education into 3 dummy variables for each education level) and divide the set into training (60%) and validation (40%) sets.
```{r}
library(dplyr)
dummy_bankdata <- dummy_columns(BankData, select_columns = 'Education')
m_bankdata <- select(dummy_bankdata,Age,Experience,Income,Family,CCAvg,Education_1,Education_2,Education_3,Mortgage,Personal.Loan,Securities.Account,CD.Account,Online,CreditCard)
m_bankdata <- m_bankdata %>% relocate(Personal.Loan,.after=last_col())#move personal.loan to the end to make our lives easier later on.
set.seed(1)
Train_Index <- sample(row.names(m_bankdata), .6*dim(m_bankdata)[1])
Val_Index <- setdiff(row.names(m_bankdata), Train_Index)
Train_Data <- m_bankdata[Train_Index,]
Validation_Data <- m_bankdata[Val_Index,]

#summary(Train_Data)
```

## Now we'll move to normalize the numeric data.

```{r}
columnsare <-c(1,2,3,4,5,9)
bankdata.norm.df <- m_bankdata
train.norm.df <- Train_Data
valid.norm.df <- Validation_Data
norm.values <- preProcess(Train_Data[,columnsare], method=c("center","scale"))

#put the normalized data back into the dataframes
train.norm.df[, columnsare] <-predict(norm.values,Train_Data[,columnsare])
valid.norm.df[, columnsare] <-predict(norm.values,Validation_Data[,columnsare])

summary(train.norm.df)
```

## Building the K-NN model

```{r}
library('FNN')
train.knn.predictors <- train.norm.df[, 1:13]
train.knn.success <-train.norm.df[,14]
valid.knn.predictors <- valid.norm.df[, 1:13]
valid.knn.success <-valid.norm.df[,14]

knn.results <- knn (train=train.knn.predictors, test=valid.knn.predictors, cl=train.knn.success, k=1, prob=TRUE)
confusionMatrix(knn.results,valid.knn.success, positive="Yes")
```
We see that the model has an accuracy of .961

## A sample customer
Now let's take a sample customer with the following characteristics: 
Age = 40,
Experience = 10,
Income = 84, 
Family = 2, 
CCAvg = 2, 
Education_1 = 0, 
Education_2 = 1,
Education_3 = 0, 
Mortgage = 0, 
Securities Account = 0, 
CD Account = 0, 
Online = 1, 
and Credit Card = 1. 

We'll evaluate him with our model.

```{r}
customertest = data.frame(Age = as.integer(40), Experience = as.integer(10), Income = as.integer(84), Family = as.integer(2), CCAvg = as.integer(2), Education1 = as.integer(0), Education2 = as.integer(1), Education3 = as.integer(0), Mortgage = as.integer(0), Securities.Account = as.integer(0), CD.Account = as.integer(0), Online = as.integer(1), CreditCard = as.integer(1))

customer.norm.df <- customertest
customer.norm.df[, columnsare]<-predict(norm.values,customertest[,columnsare])#normalize the quantitative values
```
Now that we've imported and normalized the customer's data, we will test him against our K-NN from earlier.

```{r}
customer.knn <- knn(train=train.knn.predictors, test=customer.norm.df,cl=train.knn.success,k=1, prob=TRUE)
head(customer.knn)
```
We see that the model predicts this customer would not accept a loan.

##Tuning using Validation
We will now test the performance of our model across various k's on our validation set so that we can discuss the balance for k.
```{r}
library(FNN)
accuracy.df <- data.frame(k = seq(1,14,1), accuracy = rep(0 , 14))

for(i in 1:14){
  knn.pred <- knn(train.knn.predictors,valid.knn.predictors, cl=train.knn.success,k=i)
accuracy.df[i,2] <- confusionMatrix(knn.pred, valid.knn.success)$overall[1]
  }
accuracy.df
which.max(accuracy.df$accuracy)

#Commented out - was going to do both k selection by validation and training
#library(caret)
#carettraindata<-train.norm.df[,1:13]
#carettrainclasses<-train.norm.df[,14]
#model<-train(carettraindata,carettrainclasses,"knn")
#model
```
We see from the 'which' function that the best performing k in the range of 1 to 14 is `r which.max(accuracy.df$accuracy)`. This k is the most accurate, and balances between overfitting and ignoring predictors.

```{r}
customer.knn3 <- knn(train=train.knn.predictors, test=customer.norm.df,cl=train.knn.success,k=3, prob=TRUE)#he is the same custoemr as our earlier example, so we don't need to renormalize
head(customer.knn3)
```

## Further examination of k = 3
Below is a confusion matrix of the validation data for k=3 (the best k).
```{R}
knn.k3 <- knn(train = train.knn.predictors,test=valid.knn.predictors,cl=train.knn.success,k=3, prob=TRUE)
confusionMatrix(knn.k3,valid.knn.success,)
```
We see that our accuracy is high at .9620 (and thus an error rate of 3.8%). We also have a very low false-negative. Precision (TP/(TP+FP)is low at 64% - this would be the worst metric as we are hoping to target the most responsive custoemrs, so a low precision and high false-positives (Type I errors) makes the model problematic.

## Repartitioning for a test set
```{r}
set.seed(1)
Train_Index <- sample(row.names(m_bankdata), .5*dim(m_bankdata)[1])
Val_Index <- sample(setdiff(row.names(m_bankdata),Train_Index),.3*dim(m_bankdata)[1])
Test_Index =setdiff(row.names(m_bankdata),union(Train_Index,Val_Index))

Train_Data <- m_bankdata[Train_Index,]
Validation_Data <- m_bankdata[Val_Index,]
Test_Data <- m_bankdata [Test_Index,]

norm.values3 <- preProcess(m_bankdata[,columnsare], method=c("center", "scale"))
train.norm.df3 = Train_Data
val.norm.df3 = Validation_Data
test.norm.df3 = Test_Data
train.norm.df3[, columnsare] <- predict(norm.values3, Train_Data[, columnsare])
val.norm.df3[, columnsare] <- predict(norm.values3, Validation_Data[, columnsare])
test.norm.df3[, columnsare] <- predict(norm.values3, Test_Data[, columnsare])

knn.train <- knn(train=train.norm.df3[,-14],test=train.norm.df3[,-14],cl=train.norm.df3[,14], k=3, prob=TRUE)
knn.val<- knn(train=train.norm.df3[,-14],test=val.norm.df3[,-14],cl=train.norm.df3[,14],k=3, prob=TRUE)
knn.test<- knn(train=train.norm.df3[,-14],test=test.norm.df3[,-14],cl=train.norm.df3[,14],k=3, prob=TRUE)

confusionMatrix(knn.train,train.norm.df3[,14], positive="Yes")
confusionMatrix(knn.val,val.norm.df3[,14], positive="Yes")
confusionMatrix(knn.test,test.norm.df3[,14], positive="Yes")
```
We see that the accuracies are Training: .978; Validation: .9693, and Test: .953. This makes sense as the model was trained on the higher amounts of data.